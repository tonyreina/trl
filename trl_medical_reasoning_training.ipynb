{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bb3f0d",
   "metadata": {},
   "source": [
    "# Medical Reasoning Fine-tuning with TRL and LoRA\n",
    "\n",
    "> Important\n",
    "> ---------\n",
    "> This notebook is for educational purposes only.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model for medical reasoning tasks using [**TRL (Transformers Reinforcement Learning)**](https://huggingface.co/docs/trl/index) and [**LoRA (Low-Rank Adaptation)**](https://tonyreina.github.io/lora/getting-started/what-is-lora/). If you're new to these concepts, here's what you need to know:\n",
    "\n",
    "### What is LoRA?\n",
    "[**LoRA (Low-Rank Adaptation)**](https://tonyreina.github.io/lora/getting-started/what-is-lora/) is an efficient fine-tuning technique that:\n",
    "- Freezes the original model weights and adds small trainable matrices\n",
    "- Dramatically reduces memory usage and training time\n",
    "- Achieves performance comparable to full fine-tuning\n",
    "- Creates lightweight adapters that can be easily shared and swapped\n",
    "\n",
    "### What is TRL?\n",
    "[**TRL (Transformers Reinforcement Learning)**](https://huggingface.co/docs/trl/index) is a library that:\n",
    "- Provides easy-to-use trainers for supervised fine-tuning (SFT)\n",
    "- Supports advanced training techniques like RLHF and DPO\n",
    "- Integrates seamlessly with Hugging Face transformers and PEFT\n",
    "\n",
    "### What We'll Do\n",
    "In this notebook, we'll:\n",
    "1. Load and preprocess a medical reasoning dataset\n",
    "2. Configure quantization for memory efficiency\n",
    "3. Set up LoRA adapters for the language model\n",
    "4. Train using TRL's SFTTrainer\n",
    "5. Save the trained LoRA adapter for inference\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866473b",
   "metadata": {},
   "source": [
    "## Step 1: Loading and Preprocessing the Dataset\n",
    "\n",
    "We'll use the [**FreedomIntelligence/medical-o1-reasoning-SFT** dataset](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT), which contains medical questions with complex chain-of-thought reasoning.\n",
    "\n",
    "### Dataset Format\n",
    "Each example contains:\n",
    "- **Question**: The medical question or scenario\n",
    "- **Complex_CoT**: Chain-of-thought reasoning (the model's \"thinking\" process)\n",
    "- **Response**: The final answer\n",
    "\n",
    "### Chain-of-Thought (CoT) Format\n",
    "\n",
    "1. **System prompt format**: Custom instructions followed by `/think` to enable extended thinking mode\n",
    "2. **User message**: The medical question\n",
    "3. **Assistant response**: `/think [reasoning process] [final answer]`\n",
    "\n",
    "> NOTE\n",
    "> For this example, I am using the HuggingFace SmolLM2-135M-Instruct\n",
    "> base model to fine-tune. This model **does not** have a template\n",
    "> to work with the CoT (`/think`) tags, but I include them here\n",
    "> anyway to show how that would work with such a dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e38d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: {'messages': [{'content': 'You are a medical AI assistant. When answering medical questions, use /think to show your reasoning process before providing your final answer. Structure your response as: /think [your detailed reasoning] [final answer]./think', 'role': 'system'}, {'content': 'A patient presents with microcytic hypochromic anemia, hemoglobin level of 9%, serum iron of 20 Âµg/dL, ferritin level of 800 ng/mL, and transferrin percentage saturation of 64%. Based on these laboratory findings, what is the possible diagnosis?', 'role': 'user'}, {'content': \"/think Okay, so we have a case of microcytic hypochromic anemia. That generally means the red blood cells are small and pale, which can occur in a few different conditions.\\n\\nLet's start by looking at the serum iron level. Itâ€™s reported at 20 Âµg/dL, which is definitely on the low side. Low serum iron is commonly seen in iron deficiency anemia, but it can also happen due to chronic diseases or other less common conditions.\\n\\nNext, thereâ€™s the ferritin level to consider. Itâ€™s really high at 800 ng/mL. Ferritin being high makes me think more about conditions like an inflammatory state or chronic disease, rather than iron deficiency, where ferritin would typically be low.\\n\\nNow, looking at the transferrin saturation, itâ€™s sitting at 64%, which is quite high. That's a bit puzzling because in iron deficiency anemia, I would expect this to be low. This high saturation suggests there's more than enough iron present, which doesnâ€™t match up with a straightforward iron deficiency.\\n\\nWhen I put this all together, low serum iron, high ferritin, and high transferrin saturation do not classically fit with what's expected for the typical causes of microcytic hypochromic anemia, like iron deficiency anemia or anemia of chronic disease.\\n\\nI wonder if thereâ€™s more going on here. Something like anemia of chronic disease crossed my mind at first, but with this high transferrin saturation, it doesn't really fit.\\n\\nHmm, could it be a case of sideroblastic anemia? But the high transferrin saturation seems off as sideroblastic anemia can show variable saturation, often leaning lower.\\n\\nLooking at the big picture again, excess iron is suggested by the high ferritin, so maybe something related to iron overload, like hemochromatosis. But then, the low serum iron perplexes me when it comes to hemochromatosis.\\n\\nCould this be an unusual transporter issue? Like atransferrinemia? That condition could lead to poor iron usage even though there's plenty of iron available, explaining the high saturation and ferritin.\\n\\nThat idea does make a lot of sense given what I'm seeing. Everything points to a problem with how iron is being managed in the body, rather than a classic case of deficiency or overload.\\n\\nSo, now, putting it all together, I think atransferrinemia, a rare disorder where the body doesn't handle iron transport well, fits the data. Something pretty rare, but it explains these unusual lab results well. Based on the laboratory findings and the analysis of the situation, the possible diagnosis for this patient is atransferrinemia. Atransferrinemia is a rare disorder characterized by an abnormality in iron transport, leading to inefficient iron distribution and utilization in the body. This diagnosis aligns with the lab results showing low serum iron, high ferritin levels, and high transferrin saturation. These features suggest an issue with iron transport rather than a deficiency or overload, which is consistent with atransferrinemia.\", 'role': 'assistant'}]}\n",
      "Training samples: 15763\n",
      "Test samples: 1971\n",
      "Validation samples: 1970\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "\n",
    "\n",
    "def preprocess_function(example):\n",
    "    # Format as a conversation for SFTTrainer with system prompt for CoT\n",
    "    # Note: SmolLM3-3B requires \"/think\" at the end of system prompt to enable extended thinking\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a medical AI assistant. \"\n",
    "            \"When answering medical questions, \"\n",
    "            \"use /think to show your reasoning process \"\n",
    "            \"before providing your final answer. \"\n",
    "            \"Structure your response as: /think \"\n",
    "            \"[your detailed reasoning] [final answer]./think\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": example[\"Question\"]},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"/think {example['Complex_CoT']} {example['Response']}\",\n",
    "        },\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, remove_columns=[\"Question\", \"Response\", \"Complex_CoT\"])\n",
    "\n",
    "# Split the training dataset to create train/validation/test sets\n",
    "# (80% train, 10% validation, 10% test)\n",
    "first_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=816)  # 80% train, 20% temp\n",
    "temp_dataset = first_split[\"test\"]\n",
    "second_split = temp_dataset.train_test_split(test_size=0.5, seed=816)  # Split the 20% into 10% each\n",
    "\n",
    "train_dataset = first_split[\"train\"]  # 80%\n",
    "eval_dataset = second_split[\"train\"]  # 10%\n",
    "test_dataset = second_split[\"test\"]  # 10%\n",
    "\n",
    "print(\"Sample:\", next(iter(train_dataset)))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1188",
   "metadata": {},
   "source": [
    "## Step 2: Data Collation\n",
    "\n",
    "The [**DataCollatorWithFlattening**](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator) is a special data collator from TRL that:\n",
    "- Handles variable-length sequences efficiently\n",
    "- Flattens conversation data for training\n",
    "- Optimizes memory usage during batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dd11004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithFlattening\n",
    "\n",
    "data_collator = DataCollatorWithFlattening()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadbb67c",
   "metadata": {},
   "source": [
    "## Step 3: Loading Configuration\n",
    "\n",
    "We use a [YAML configuration file](config.yaml) to manage training parameters. This approach:\n",
    "- Keeps settings organized and version-controlled\n",
    "- Makes it easy to experiment with different hyperparameters\n",
    "- Allows reproducible training runs\n",
    "\n",
    "### Key Parameters Explained:\n",
    "- **base_model_name**: The foundation model to fine-tune\n",
    "- **lora_rank**: Controls the size of LoRA adapters (higher = more parameters)\n",
    "- **lora_alpha**: Scaling factor for LoRA updates (affects learning strength)\n",
    "- **batch_size**: Number of samples processed together\n",
    "- **epochs_to_train**: Number of complete passes through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3429860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c56d3d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: microsoft/Phi-4-mini-instruct\n",
      "LoRA adapter directory will be saved to: lora_adapter/microsoft/Phi-4-mini-instruct\n",
      "LoRA rank is 16 and LoRA alpha is 32\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "MODEL_NAME = config[\"base_model_name\"]\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "\n",
    "adapter_dir = join(config[\"adapter_dir_prefix\"], MODEL_NAME)\n",
    "print(f\"LoRA adapter directory will be saved to: {adapter_dir}\")\n",
    "\n",
    "lora_rank = config[\"lora_rank\"]\n",
    "lora_alpha = config[\"lora_alpha\"]\n",
    "print(f\"LoRA rank is {lora_rank} and LoRA alpha is {lora_alpha}\")\n",
    "\n",
    "batch_size = int(config[\"batch_size\"])\n",
    "epochs_to_train = int(config[\"epochs_to_train\"])\n",
    "max_output_length = int(config[\"max_output_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520b0a1",
   "metadata": {},
   "source": [
    "## Step 4: Setting Up Model Components\n",
    "\n",
    "Now we'll import the essential libraries:\n",
    "\n",
    "- [**PyTorch**](https://pytorch.org): The underlying tensor computation framework\n",
    "- [**PEFT (LoraConfig)**](https://huggingface.co/docs/peft/en/index): Handles parameter-efficient fine-tuning\n",
    "- [**Transformers**](https://huggingface.co/docs/transformers/en/index): Provides the model and tokenizer classes\n",
    "- [**BitsAndBytesConfig**](https://huggingface.co/docs/bitsandbytes/en/index): Enables memory-efficient quantization\n",
    "- [**TRL**](https://huggingface.co/docs/trl/index): Provides the specialized trainer for supervised fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0998604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64344f8",
   "metadata": {},
   "source": [
    "## Step 5: Quantization Configuration\n",
    "\n",
    "**Quantization** with BitsAndBytes reduces memory usage by representing model weights with fewer bits:\n",
    "\n",
    "- **load_in_4bit**: Use 4-bit precision instead of 16/32-bit (4x memory reduction!)\n",
    "- **bnb_4bit_compute_dtype**: Use bfloat16 for computations (stable training)\n",
    "- **bnb_4bit_use_double_quant**: Apply quantization twice for even more savings  \n",
    "- **bnb_4bit_quant_type**: \"nf4\" is an optimized 4-bit format\n",
    "\n",
    "The base model is quantized using BitsandBytes, but the fine-tuned\n",
    "LoRA weights and gradients are at `bfloat16` precision.\n",
    "\n",
    "This allows us to train larger models on consumer GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2a4b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Changed from float16 to bfloat16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69632a48",
   "metadata": {},
   "source": [
    "## Step 6: Loading Model and Tokenizer\n",
    "\n",
    "Here we load the base model with several optimization flags:\n",
    "\n",
    "- **quantization_config**: Apply the 4-bit quantization we configured\n",
    "- **device_map=\"auto\"**: Automatically distribute model across available GPUs\n",
    "- **attn_implementation=\"flash_attention_2\"**: Use optimized attention for speed\n",
    "- **local_files_only**: Use cached models when available\n",
    "\n",
    "The tokenizer converts text to tokens the model can understand. We set `pad_token = eos_token` because some models don't have a dedicated padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee208692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cfff46e1d040b0ac19ecf7886102ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,  # float16 to bfloat16\n",
    "    use_cache=True,  # Whether to cache attention outputs to speed up inference\n",
    "    quantization_config=bnb_config,\n",
    "    local_files_only=True,  # Use cache first\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    local_files_only=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c0e85",
   "metadata": {},
   "source": [
    "## Step 7: LoRA Configuration\n",
    "\n",
    "Now we configure the **LoRA adapter** - this is the heart of parameter-efficient fine-tuning!\n",
    "\n",
    "I have a more complete description of the LoRA approach [here](https://tonyreina.github.io/lora/getting-started/what-is-lora/).\n",
    "\n",
    "### LoRA Parameters Explained:\n",
    "- **r (rank)**: Size of the low-rank matrices\n",
    "- **lora_alpha**: Scaling factor for LoRA updates (higher = stronger adaptation)\n",
    "- **lora_dropout**: Prevents overfitting in the LoRA layers\n",
    "- **target_modules**: Which parts of the model to adapt (attention layers are most effective)\n",
    "\n",
    "### Why These Modules?\n",
    "We target the attention projection layers because they:\n",
    "- Control how the model focuses on different parts of the input\n",
    "- Are most impactful for learning new reasoning patterns\n",
    "- Provide good performance-to-parameter ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70becb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"self_attn.q_proj\", \"self_attn.v_proj\", \"self_attn.k_proj\", \"self_attn.o_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434daf1",
   "metadata": {},
   "source": [
    "## Step 8: Training Configuration\n",
    "\n",
    "The [**SFTConfig**](https://huggingface.co/docs/trl/en/sft_trainer) defines how we want to train our model:\n",
    "\n",
    "### Memory & Performance:\n",
    "- **gradient_accumulation_steps=4**: Process 4 batches before updating (saves memory)\n",
    "- **bf16=True**: Use bfloat16 precision (faster, stable training)\n",
    "- **gradient_checkpointing=True**: Trade compute for memory (essential for large models)\n",
    "\n",
    "### Training Strategy:\n",
    "- **completion_only_loss=True**: Only train on assistant responses, not user questions\n",
    "- **loss_type=\"dft\"**: Dynamic fine-tuning loss (TRL's improved loss function)\n",
    "\n",
    "> NOTE\n",
    ">\n",
    "> [Dynamic Fine Tuning](https://huggingface.co/papers/2508.05629) is a modification\n",
    "> to the traditional `log_loss` function used to train & fine-tune LLMs.\n",
    "> It dynamically scales the `log_loss` by the probability of the token.\n",
    "> In the [original paper](https://arxiv.org/pdf/2508.05629) it was shown\n",
    "> to be as effective as RL approaches, such as PPO, GRPO, and DPO.\n",
    "\n",
    "### Monitoring:\n",
    "- **eval_strategy/save_strategy**: Save and evaluate every 100 steps\n",
    "- **logging_steps=50**: Log training metrics frequently\n",
    "\n",
    "> TIP\n",
    ">\n",
    "> The logs are sent to a local [MLFlow](https://mlflow.org) database.\n",
    "> You can monitor the training in realtime by starting the\n",
    "> MLFlow server (`pixi run -e cuda mlflow ui`) and opening\n",
    "> the browser to `http://localhost:5000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6295e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SFT training parameters\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=config[\"training_results_dir\"],\n",
    "    num_train_epochs=epochs_to_train,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    max_length=max_output_length,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    loss_type=\"dft\",  # Dynamic fine tuning\n",
    "    completion_only_loss=True,  # Train only on assistant responses\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",  # Cosine annealing with warm restarts for simulated annealing\n",
    "    lr_scheduler_kwargs={\n",
    "        \"num_cycles\": 2.0,  # Number of restart cycles (2.0 = 2 complete cosine waves with restarts)\n",
    "    },\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,  # Keep all columns for manual evaluation after training\n",
    "    report_to=\"mlflow\",  # Use MLflow to track training experiments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f379e23",
   "metadata": {},
   "source": [
    "## Step 9: Creating the SFT Trainer\n",
    "\n",
    "The [**SFTTrainer** (Supervised Fine-Tuning Trainer)](https://huggingface.co/docs/trl/en/sft_trainer) is TRL's specialized trainer that:\n",
    "- Handles conversation formatting automatically\n",
    "- Integrates with PEFT for LoRA training  \n",
    "- Provides optimized training loops for language model fine-tuning\n",
    "- Supports advanced features like completion-only training\n",
    "\n",
    "This single trainer handles all the complexity of modern LLM fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b668b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19e9d8",
   "metadata": {},
   "source": [
    "## Step 10: Memory Usage Check\n",
    "\n",
    "Before training, let's check our GPU memory usage to ensure we have enough resources:\n",
    "\n",
    "- **Memory allocated**: Currently used GPU memory\n",
    "- **Memory reserved**: Memory reserved by PyTorch for operations\n",
    "- **Memory available**: Total GPU memory capacity\n",
    "\n",
    "This helps us verify that our quantization and optimization settings are working correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c750bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory allocated: 5.79 GB\n",
      "GPU Memory reserved: 9.15 GB\n",
      "GPU Memory available: 47.35 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory usage before training\n",
    "GB = 2**30\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / GB:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9734e",
   "metadata": {},
   "source": [
    "## Step 11: Start Training! ðŸš€\n",
    "\n",
    "This is where the magic happens! The trainer will:\n",
    "\n",
    "1. **Forward pass**: Run examples through the model\n",
    "2. **Compute loss**: Measure prediction accuracy  \n",
    "3. **Backward pass**: Calculate gradients\n",
    "4. **Update LoRA weights**: Apply parameter updates (only ~0.1% of total parameters!)\n",
    "5. **Evaluate**: Test on validation data periodically\n",
    "6. **Save checkpoints**: Store progress for recovery\n",
    "\n",
    "**Note**: Training progress will show loss decreasing and evaluation metrics improving. The beauty of LoRA is that we're only updating a tiny fraction of the model's parameters while achieving full fine-tuning performance!\n",
    "\n",
    "### Track experiment using MLFlow\n",
    "\n",
    "On the commandline run `pixi run -e cuda mlflow ui` to start the MLFlow tracking server. This will allow you to monitor the training in realtime at [http://localhost:5000](http://localhost:5000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19865571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/22 21:07:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
      "2026/01/22 21:07:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
      "2026/01/22 21:07:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
      "2026/01/22 21:07:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
      "2026/01/22 21:07:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
      "2026/01/22 21:07:20 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
      "2026/01/22 21:07:20 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/22 21:07:20 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/22 21:07:20 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/22 21:07:20 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/22 21:07:20 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/22 21:07:20 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3944' max='3944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3944/3944 4:54:39, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.141400</td>\n",
       "      <td>0.114936</td>\n",
       "      <td>0.878778</td>\n",
       "      <td>1017650.000000</td>\n",
       "      <td>0.575286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.044849</td>\n",
       "      <td>0.268543</td>\n",
       "      <td>2035416.000000</td>\n",
       "      <td>0.594630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.039879</td>\n",
       "      <td>0.246013</td>\n",
       "      <td>3062739.000000</td>\n",
       "      <td>0.602813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.040773</td>\n",
       "      <td>0.246318</td>\n",
       "      <td>4092207.000000</td>\n",
       "      <td>0.607576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.039847</td>\n",
       "      <td>0.239885</td>\n",
       "      <td>5110860.000000</td>\n",
       "      <td>0.609942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.037489</td>\n",
       "      <td>0.223873</td>\n",
       "      <td>6132648.000000</td>\n",
       "      <td>0.610539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.038177</td>\n",
       "      <td>0.217915</td>\n",
       "      <td>7159054.000000</td>\n",
       "      <td>0.613317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.038660</td>\n",
       "      <td>0.214454</td>\n",
       "      <td>8182179.000000</td>\n",
       "      <td>0.614343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.037647</td>\n",
       "      <td>0.211119</td>\n",
       "      <td>9212964.000000</td>\n",
       "      <td>0.614986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.038232</td>\n",
       "      <td>0.211039</td>\n",
       "      <td>10235964.000000</td>\n",
       "      <td>0.615383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>0.214438</td>\n",
       "      <td>11256838.000000</td>\n",
       "      <td>0.615597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.037959</td>\n",
       "      <td>0.210508</td>\n",
       "      <td>12289195.000000</td>\n",
       "      <td>0.617340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.036169</td>\n",
       "      <td>0.202701</td>\n",
       "      <td>13313449.000000</td>\n",
       "      <td>0.617344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.037269</td>\n",
       "      <td>0.207077</td>\n",
       "      <td>14346936.000000</td>\n",
       "      <td>0.617834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.035870</td>\n",
       "      <td>0.199717</td>\n",
       "      <td>15358903.000000</td>\n",
       "      <td>0.617682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.036601</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>16387831.000000</td>\n",
       "      <td>0.618095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.037472</td>\n",
       "      <td>0.207612</td>\n",
       "      <td>17416307.000000</td>\n",
       "      <td>0.618253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.036797</td>\n",
       "      <td>0.202431</td>\n",
       "      <td>18434980.000000</td>\n",
       "      <td>0.618223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.036915</td>\n",
       "      <td>0.203524</td>\n",
       "      <td>19454017.000000</td>\n",
       "      <td>0.618441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.038448</td>\n",
       "      <td>0.209959</td>\n",
       "      <td>20474367.000000</td>\n",
       "      <td>0.618708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.037115</td>\n",
       "      <td>0.203083</td>\n",
       "      <td>21502646.000000</td>\n",
       "      <td>0.618527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.036762</td>\n",
       "      <td>0.202224</td>\n",
       "      <td>22530539.000000</td>\n",
       "      <td>0.618522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.036999</td>\n",
       "      <td>0.201663</td>\n",
       "      <td>23552678.000000</td>\n",
       "      <td>0.618585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>0.037725</td>\n",
       "      <td>0.206249</td>\n",
       "      <td>24571812.000000</td>\n",
       "      <td>0.618687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.036645</td>\n",
       "      <td>0.201027</td>\n",
       "      <td>25595937.000000</td>\n",
       "      <td>0.618512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.036954</td>\n",
       "      <td>0.202797</td>\n",
       "      <td>26622834.000000</td>\n",
       "      <td>0.618627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.036962</td>\n",
       "      <td>0.202715</td>\n",
       "      <td>27636573.000000</td>\n",
       "      <td>0.618647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.036603</td>\n",
       "      <td>0.200913</td>\n",
       "      <td>28666791.000000</td>\n",
       "      <td>0.618556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.036812</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>29694407.000000</td>\n",
       "      <td>0.618629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.201380</td>\n",
       "      <td>30710112.000000</td>\n",
       "      <td>0.618665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.036721</td>\n",
       "      <td>0.200761</td>\n",
       "      <td>31731379.000000</td>\n",
       "      <td>0.618597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.036762</td>\n",
       "      <td>0.201059</td>\n",
       "      <td>32758933.000000</td>\n",
       "      <td>0.618662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.036795</td>\n",
       "      <td>0.202079</td>\n",
       "      <td>33783015.000000</td>\n",
       "      <td>0.618586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.036871</td>\n",
       "      <td>0.201390</td>\n",
       "      <td>34809103.000000</td>\n",
       "      <td>0.618705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.036740</td>\n",
       "      <td>0.201044</td>\n",
       "      <td>35832270.000000</td>\n",
       "      <td>0.618592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.036792</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>36857564.000000</td>\n",
       "      <td>0.618648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.036683</td>\n",
       "      <td>0.200525</td>\n",
       "      <td>37881873.000000</td>\n",
       "      <td>0.618610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>0.200504</td>\n",
       "      <td>38902832.000000</td>\n",
       "      <td>0.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.036715</td>\n",
       "      <td>0.200979</td>\n",
       "      <td>39924040.000000</td>\n",
       "      <td>0.618656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3944, training_loss=0.040891241561810336, metrics={'train_runtime': 17681.9589, 'train_samples_per_second': 3.566, 'train_steps_per_second': 0.223, 'total_flos': 7.810918543234253e+17, 'train_loss': 0.040891241561810336, 'entropy': 0.20005356283546183, 'num_tokens': 40371876.0, 'mean_token_accuracy': 0.62258922089042, 'epoch': 4.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb283ef7",
   "metadata": {},
   "source": [
    "## Step 12: Save the LoRA Adapter\n",
    "\n",
    "After training completes, we save our LoRA adapter:\n",
    "\n",
    "### What Gets Saved:\n",
    "- **LoRA weight matrices**: The small adapters we trained (~few MB)\n",
    "- **Adapter configuration**: Settings like rank, alpha, target modules\n",
    "- **Tokenizer**: Ensures consistency during inference\n",
    "\n",
    "### Why This is Amazing:\n",
    "- The base model stays unchanged (no need to duplicate GBs of weights)\n",
    "- Multiple LoRA adapters can be created for different tasks\n",
    "- Adapters can be easily shared, version-controlled, and swapped\n",
    "- You can even combine multiple LoRA adapters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60ac35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA adapter to lora_adapter/microsoft/Phi-4-mini-instruct\n",
      "LoRA adapter saved successfully to lora_adapter/microsoft/Phi-4-mini-instruct!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lora_adapter/microsoft/Phi-4-mini-instruct/tokenizer_config.json',\n",
       " 'lora_adapter/microsoft/Phi-4-mini-instruct/special_tokens_map.json',\n",
       " 'lora_adapter/microsoft/Phi-4-mini-instruct/chat_template.jinja',\n",
       " 'lora_adapter/microsoft/Phi-4-mini-instruct/vocab.json',\n",
       " 'lora_adapter/microsoft/Phi-4-mini-instruct/merges.txt',\n",
       " 'lora_adapter/microsoft/Phi-4-mini-instruct/added_tokens.json',\n",
       " 'lora_adapter/microsoft/Phi-4-mini-instruct/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the LoRA adapter\n",
    "print(f\"Saving LoRA adapter to {adapter_dir}\")\n",
    "\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "print(f\"LoRA adapter saved successfully to {adapter_dir}!\")\n",
    "tokenizer.save_pretrained(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edcce60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_dataset_for_evaluation.<locals>.tokenize_function at 0x74202c1c4ea0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfb7a7fbb88472b8c8eb279d953f587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset for evaluation:   0%|          | 0/1971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 02:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Test Results:\n",
      "   eval_loss: 0.0366\n",
      "   eval_runtime: 128.1027\n",
      "   eval_samples_per_second: 15.3860\n",
      "   eval_steps_per_second: 1.9280\n",
      "   eval_entropy: 0.2005\n",
      "   eval_num_tokens: 40371876.0000\n",
      "   eval_mean_token_accuracy: 0.6186\n",
      "   epoch: 4.0000\n"
     ]
    }
   ],
   "source": [
    "# Process test dataset to match the expected format for evaluation\n",
    "# Apply the same chat template processing that SFTTrainer uses\n",
    "def process_dataset_for_evaluation(dataset, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        # Apply chat template to format messages\n",
    "        formatted_texts = []\n",
    "        for messages in examples[\"messages\"]:\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            formatted_texts.append(formatted_text)\n",
    "\n",
    "        # Tokenize the formatted text\n",
    "        tokenized = tokenizer(\n",
    "            formatted_texts,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=trainer.args.max_length,\n",
    "            return_overflowing_tokens=False,\n",
    "        )\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    # Process the dataset\n",
    "    processed_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing dataset for evaluation\",\n",
    "    )\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "processed_test_dataset = process_dataset_for_evaluation(test_dataset, tokenizer)\n",
    "\n",
    "test_results = trainer.evaluate(processed_test_dataset)\n",
    "print(\"ðŸŽ¯ Test Results:\")\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a2390d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now run the notebook `trl_medical_reasoning_inference.ipynb` to use the LoRA fine-tuned model.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Now run the notebook `trl_medical_reasoning_inference.ipynb` to use the LoRA fine-tuned model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cf394",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. âœ… Learned about LoRA and TRL\n",
    "2. âœ… Loaded and preprocessed medical reasoning data  \n",
    "3. âœ… Set up memory-efficient quantization\n",
    "4. âœ… Configured LoRA adapters for parameter-efficient training\n",
    "5. âœ… Fine-tuned a language model using TRL's SFTTrainer\n",
    "6. âœ… Saved your trained LoRA adapter\n",
    "\n",
    "### Next Steps:\n",
    "- Use the inference notebook to test your fine-tuned model\n",
    "- Experiment with different LoRA ranks and alphas\n",
    "- Try training on different datasets\n",
    "- Combine multiple LoRA adapters for multi-task models\n",
    "\n",
    "### Key Takeaways:\n",
    "- **LoRA** enables efficient fine-tuning with minimal memory\n",
    "- **TRL** provides state-of-the-art training techniques\n",
    "- **Quantization** makes large models accessible on consumer hardware\n",
    "- **Parameter-efficient fine-tuning** is the future of model customization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
