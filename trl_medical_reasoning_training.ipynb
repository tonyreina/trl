{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bb3f0d",
   "metadata": {},
   "source": [
    "# Medical Reasoning Fine-tuning with TRL and LoRA\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model for medical reasoning tasks using **TRL (Transformers Reinforcement Learning)** and **LoRA (Low-Rank Adaptation)**. If you're new to these concepts, here's what you need to know:\n",
    "\n",
    "### What is LoRA?\n",
    "**LoRA (Low-Rank Adaptation)** is an efficient fine-tuning technique that:\n",
    "- Freezes the original model weights and adds small trainable matrices\n",
    "- Dramatically reduces memory usage and training time\n",
    "- Achieves performance comparable to full fine-tuning\n",
    "- Creates lightweight adapters that can be easily shared and swapped\n",
    "\n",
    "### What is TRL?\n",
    "**TRL (Transformers Reinforcement Learning)** is a library that:\n",
    "- Provides easy-to-use trainers for supervised fine-tuning (SFT)\n",
    "- Supports advanced training techniques like RLHF and DPO\n",
    "- Integrates seamlessly with Hugging Face transformers and PEFT\n",
    "\n",
    "### What We'll Do\n",
    "In this notebook, we'll:\n",
    "1. Load and preprocess a medical reasoning dataset\n",
    "2. Configure quantization for memory efficiency\n",
    "3. Set up LoRA adapters for the language model\n",
    "4. Train using TRL's SFTTrainer\n",
    "5. Save the trained LoRA adapter for inference\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866473b",
   "metadata": {},
   "source": [
    "## Step 1: Loading and Preprocessing the Dataset\n",
    "\n",
    "We'll use the **FreedomIntelligence/medical-o1-reasoning-SFT** dataset, which contains medical questions with complex chain-of-thought reasoning.\n",
    "\n",
    "### Dataset Format\n",
    "Each example contains:\n",
    "- **Question**: The medical question or scenario\n",
    "- **Complex_CoT**: Chain-of-thought reasoning (the model's \"thinking\" process)\n",
    "- **Response**: The final answer\n",
    "\n",
    "### Why This Format?\n",
    "We format the data as conversations with a special `<think>` tag to teach the model to:\n",
    "1. Show its reasoning process explicitly\n",
    "2. Provide clear, structured answers\n",
    "3. Think step-by-step through complex medical scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "\n",
    "\n",
    "def preprocess_function(example):\n",
    "    # Format as a conversation for SFTTrainer\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"Question\"]},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"<think>{example['Complex_CoT']}</think>{example['Response']}\",\n",
    "        },\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, remove_columns=[\"Question\", \"Response\", \"Complex_CoT\"])\n",
    "\n",
    "# Split the training dataset to create train/validation/test sets\n",
    "# (80% train, 10% validation, 10% test)\n",
    "first_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=816)  # 80% train, 20% temp\n",
    "temp_dataset = first_split[\"test\"]\n",
    "second_split = temp_dataset.train_test_split(test_size=0.5, seed=816)  # Split the 20% into 10% each\n",
    "\n",
    "train_dataset = first_split[\"train\"]  # 80%\n",
    "eval_dataset = second_split[\"train\"]  # 10%\n",
    "test_dataset = second_split[\"test\"]  # 10%\n",
    "\n",
    "print(\"Sample:\", next(iter(train_dataset)))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1188",
   "metadata": {},
   "source": [
    "## Step 2: Data Collation\n",
    "\n",
    "The **DataCollatorWithFlattening** is a special data collator from TRL that:\n",
    "- Handles variable-length sequences efficiently\n",
    "- Flattens conversation data for training\n",
    "- Optimizes memory usage during batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd11004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithFlattening\n",
    "\n",
    "data_collator = DataCollatorWithFlattening()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadbb67c",
   "metadata": {},
   "source": [
    "## Step 3: Loading Configuration\n",
    "\n",
    "We use a YAML configuration file to manage training parameters. This approach:\n",
    "- Keeps settings organized and version-controlled\n",
    "- Makes it easy to experiment with different hyperparameters\n",
    "- Allows reproducible training runs\n",
    "\n",
    "### Key Parameters Explained:\n",
    "- **base_model_name**: The foundation model to fine-tune\n",
    "- **lora_rank**: Controls the size of LoRA adapters (higher = more parameters)\n",
    "- **lora_alpha**: Scaling factor for LoRA updates (affects learning strength)\n",
    "- **batch_size**: Number of samples processed together\n",
    "- **epochs_to_train**: Number of complete passes through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3429860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from config.yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "MODEL_NAME = config[\"base_model_name\"]\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "\n",
    "adapter_dir = join(config[\"adapter_dir_prefix\"], MODEL_NAME)\n",
    "print(f\"LoRA adapter directory will be saved to: {adapter_dir}\")\n",
    "\n",
    "lora_rank = config[\"lora_rank\"]\n",
    "lora_alpha = config[\"lora_alpha\"]\n",
    "print(f\"LoRA rank is {lora_rank} and LoRA alpha is {lora_alpha}\")\n",
    "\n",
    "batch_size = int(config[\"batch_size\"])\n",
    "epochs_to_train = int(config[\"epochs_to_train\"])\n",
    "max_output_length = int(config[\"max_output_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520b0a1",
   "metadata": {},
   "source": [
    "## Step 4: Setting Up Model Components\n",
    "\n",
    "Now we'll import the essential libraries:\n",
    "\n",
    "- **PyTorch**: The underlying tensor computation framework\n",
    "- **PEFT (LoraConfig)**: Handles parameter-efficient fine-tuning\n",
    "- **Transformers**: Provides the model and tokenizer classes\n",
    "- **BitsAndBytesConfig**: Enables memory-efficient quantization\n",
    "- **TRL**: Provides the specialized trainer for supervised fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0998604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64344f8",
   "metadata": {},
   "source": [
    "## Step 5: Quantization Configuration\n",
    "\n",
    "**Quantization** reduces memory usage by representing model weights with fewer bits:\n",
    "\n",
    "- **load_in_4bit**: Use 4-bit precision instead of 16/32-bit (4x memory reduction!)\n",
    "- **bnb_4bit_compute_dtype**: Use bfloat16 for computations (stable training)\n",
    "- **bnb_4bit_use_double_quant**: Apply quantization twice for even more savings  \n",
    "- **bnb_4bit_quant_type**: \"nf4\" is an optimized 4-bit format\n",
    "\n",
    "This allows us to train larger models on consumer GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a4b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Changed from float16 to bfloat16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69632a48",
   "metadata": {},
   "source": [
    "## Step 6: Loading Model and Tokenizer\n",
    "\n",
    "Here we load the base model with several optimization flags:\n",
    "\n",
    "- **quantization_config**: Apply the 4-bit quantization we configured\n",
    "- **device_map=\"auto\"**: Automatically distribute model across available GPUs\n",
    "- **attn_implementation=\"flash_attention_2\"**: Use optimized attention for speed\n",
    "- **local_files_only**: Use cached models when available\n",
    "\n",
    "The tokenizer converts text to tokens the model can understand. We set `pad_token = eos_token` because some models don't have a dedicated padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee208692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,  # float16 to bfloat16\n",
    "    use_cache=True,  # Whether to cache attention outputs to speed up inference\n",
    "    quantization_config=bnb_config,\n",
    "    local_files_only=True,  # Use cache first\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    local_files_only=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c0e85",
   "metadata": {},
   "source": [
    "## Step 7: LoRA Configuration\n",
    "\n",
    "Now we configure the **LoRA adapter** - this is the heart of parameter-efficient fine-tuning!\n",
    "\n",
    "### LoRA Parameters Explained:\n",
    "- **r (rank)**: Size of the low-rank matrices\n",
    "- **lora_alpha**: Scaling factor for LoRA updates (higher = stronger adaptation)\n",
    "- **lora_dropout**: Prevents overfitting in the LoRA layers\n",
    "- **target_modules**: Which parts of the model to adapt (attention layers are most effective)\n",
    "\n",
    "### Why These Modules?\n",
    "We target the attention projection layers because they:\n",
    "- Control how the model focuses on different parts of the input\n",
    "- Are most impactful for learning new reasoning patterns\n",
    "- Provide good performance-to-parameter ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70becb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"self_attn.q_proj\", \"self_attn.v_proj\", \"self_attn.k_proj\", \"self_attn.o_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434daf1",
   "metadata": {},
   "source": [
    "## Step 8: Training Configuration\n",
    "\n",
    "The **SFTConfig** defines how we want to train our model:\n",
    "\n",
    "### Memory & Performance:\n",
    "- **gradient_accumulation_steps=4**: Process 4 batches before updating (saves memory)\n",
    "- **bf16=True**: Use bfloat16 precision (faster, stable training)\n",
    "- **gradient_checkpointing=True**: Trade compute for memory (essential for large models)\n",
    "\n",
    "### Training Strategy:\n",
    "- **completion_only_loss=True**: Only train on assistant responses, not user questions\n",
    "- **loss_type=\"dft\"**: Dynamic fine-tuning loss (TRL's improved loss function)\n",
    "\n",
    "### Monitoring:\n",
    "- **eval_strategy/save_strategy**: Save and evaluate every 100 steps\n",
    "- **logging_steps=50**: Log training metrics frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6295e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SFT training parameters\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=epochs_to_train,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    max_length=max_output_length,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    loss_type=\"dft\",  # Dynamic fine tuning\n",
    "    completion_only_loss=True,  # Train only on assistant responses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f379e23",
   "metadata": {},
   "source": [
    "## Step 9: Creating the SFT Trainer\n",
    "\n",
    "The **SFTTrainer** (Supervised Fine-Tuning Trainer) is TRL's specialized trainer that:\n",
    "- Handles conversation formatting automatically\n",
    "- Integrates with PEFT for LoRA training  \n",
    "- Provides optimized training loops for language model fine-tuning\n",
    "- Supports advanced features like completion-only training\n",
    "\n",
    "This single trainer handles all the complexity of modern LLM fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b668b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19e9d8",
   "metadata": {},
   "source": [
    "## Step 10: Memory Usage Check\n",
    "\n",
    "Before training, let's check our GPU memory usage to ensure we have enough resources:\n",
    "\n",
    "- **Memory allocated**: Currently used GPU memory\n",
    "- **Memory reserved**: Memory reserved by PyTorch for operations\n",
    "- **Memory available**: Total GPU memory capacity\n",
    "\n",
    "This helps us verify that our quantization and optimization settings are working correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c750bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage before training\n",
    "GB = 2**30\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / GB:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9734e",
   "metadata": {},
   "source": [
    "## Step 11: Start Training! ðŸš€\n",
    "\n",
    "This is where the magic happens! The trainer will:\n",
    "\n",
    "1. **Forward pass**: Run examples through the model\n",
    "2. **Compute loss**: Measure prediction accuracy  \n",
    "3. **Backward pass**: Calculate gradients\n",
    "4. **Update LoRA weights**: Apply parameter updates (only ~0.1% of total parameters!)\n",
    "5. **Evaluate**: Test on validation data periodically\n",
    "6. **Save checkpoints**: Store progress for recovery\n",
    "\n",
    "**Note**: Training progress will show loss decreasing and evaluation metrics improving. The beauty of LoRA is that we're only updating a tiny fraction of the model's parameters while achieving full fine-tuning performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19865571",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb283ef7",
   "metadata": {},
   "source": [
    "## Step 12: Save the LoRA Adapter\n",
    "\n",
    "After training completes, we save our LoRA adapter:\n",
    "\n",
    "### What Gets Saved:\n",
    "- **LoRA weight matrices**: The small adapters we trained (~few MB)\n",
    "- **Adapter configuration**: Settings like rank, alpha, target modules\n",
    "- **Tokenizer**: Ensures consistency during inference\n",
    "\n",
    "### Why This is Amazing:\n",
    "- The base model stays unchanged (no need to duplicate GBs of weights)\n",
    "- Multiple LoRA adapters can be created for different tasks\n",
    "- Adapters can be easily shared, version-controlled, and swapped\n",
    "- You can even combine multiple LoRA adapters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "print(f\"Saving LoRA adapter to {adapter_dir}\")\n",
    "\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "print(f\"LoRA adapter saved successfully to {adapter_dir}!\")\n",
    "tokenizer.save_pretrained(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2390d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Now run the notebook `trl_medical_reasoning_inference.ipynb` to use the LoRA fine-tuned model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cf394",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. âœ… Learned about LoRA and TRL\n",
    "2. âœ… Loaded and preprocessed medical reasoning data  \n",
    "3. âœ… Set up memory-efficient quantization\n",
    "4. âœ… Configured LoRA adapters for parameter-efficient training\n",
    "5. âœ… Fine-tuned a language model using TRL's SFTTrainer\n",
    "6. âœ… Saved your trained LoRA adapter\n",
    "\n",
    "### Next Steps:\n",
    "- Use the inference notebook to test your fine-tuned model\n",
    "- Experiment with different LoRA ranks and alphas\n",
    "- Try training on different datasets\n",
    "- Combine multiple LoRA adapters for multi-task models\n",
    "\n",
    "### Key Takeaways:\n",
    "- **LoRA** enables efficient fine-tuning with minimal memory\n",
    "- **TRL** provides state-of-the-art training techniques\n",
    "- **Quantization** makes large models accessible on consumer hardware\n",
    "- **Parameter-efficient fine-tuning** is the future of model customization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
