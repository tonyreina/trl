{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bb3f0d",
   "metadata": {},
   "source": [
    "# Medical Reasoning Fine-tuning with TRL and LoRA\n",
    "\n",
    "> Important\n",
    "> ---------\n",
    "> This notebook is for educational purposes only.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to fine-tune a language model for medical reasoning tasks using **TRL (Transformers Reinforcement Learning)** and **LoRA (Low-Rank Adaptation)** with **SmolLM3-3B's native CoT capabilities**. If you're new to these concepts, here's what you need to know:\n",
    "\n",
    "### What is LoRA?\n",
    "**LoRA (Low-Rank Adaptation)** is an efficient fine-tuning technique that:\n",
    "- Freezes the original model weights and adds small trainable matrices\n",
    "- Dramatically reduces memory usage and training time\n",
    "- Achieves performance comparable to full fine-tuning\n",
    "- Creates lightweight adapters that can be easily shared and swapped\n",
    "\n",
    "### What is TRL?\n",
    "**TRL (Transformers Reinforcement Learning)** is a library that:\n",
    "- Provides easy-to-use trainers for supervised fine-tuning (SFT)\n",
    "- Supports advanced training techniques like RLHF and DPO\n",
    "- Integrates seamlessly with Hugging Face transformers and PEFT\n",
    "\n",
    "### What We'll Do\n",
    "In this notebook, we'll:\n",
    "1. Load and preprocess a medical reasoning dataset\n",
    "2. Configure quantization for memory efficiency\n",
    "3. Set up LoRA adapters for the language model\n",
    "4. Train using TRL's SFTTrainer\n",
    "5. Save the trained LoRA adapter for inference\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866473b",
   "metadata": {},
   "source": [
    "## Step 1: Loading and Preprocessing the Dataset\n",
    "\n",
    "We'll use the [**FreedomIntelligence/medical-o1-reasoning-SFT** dataset](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT), which contains medical questions with complex chain-of-thought reasoning.\n",
    "\n",
    "### Dataset Format\n",
    "Each example contains:\n",
    "- **Question**: The medical question or scenario\n",
    "- **Complex_CoT**: Chain-of-thought reasoning (the model's \"thinking\" process)\n",
    "- **Response**: The final answer\n",
    "\n",
    "### Chain-of-Thought (CoT) Format\n",
    "\n",
    "1. **System prompt format**: Custom instructions followed by `/think` to enable extended thinking mode\n",
    "2. **User message**: The medical question\n",
    "3. **Assistant response**: `/think [reasoning process] [final answer]`\n",
    "\n",
    "> NOTE\n",
    "> For this example, I am using the HuggingFace SmolLM2-135M-Instruct\n",
    "> base model to fine-tune. This model does not have a template\n",
    "> to work with the CoT (`/think`) tags, but I include them here\n",
    "> anyway to show how that would work with such a dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "\n",
    "\n",
    "def preprocess_function(example):\n",
    "    # Format as a conversation for SFTTrainer with system prompt for CoT\n",
    "    # Note: SmolLM3-3B requires \"/think\" at the end of system prompt to enable extended thinking\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a medical AI assistant. \"\n",
    "            \"When answering medical questions, \"\n",
    "            \"use /think to show your reasoning process \"\n",
    "            \"before providing your final answer. \"\n",
    "            \"Structure your response as: /think \"\n",
    "            \"[your detailed reasoning] [final answer]./think\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": example[\"Question\"]},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"/think {example['Complex_CoT']} {example['Response']}\",\n",
    "        },\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, remove_columns=[\"Question\", \"Response\", \"Complex_CoT\"])\n",
    "\n",
    "# Split the training dataset to create train/validation/test sets\n",
    "# (80% train, 10% validation, 10% test)\n",
    "first_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=816)  # 80% train, 20% temp\n",
    "temp_dataset = first_split[\"test\"]\n",
    "second_split = temp_dataset.train_test_split(test_size=0.5, seed=816)  # Split the 20% into 10% each\n",
    "\n",
    "train_dataset = first_split[\"train\"]  # 80%\n",
    "eval_dataset = second_split[\"train\"]  # 10%\n",
    "test_dataset = second_split[\"test\"]  # 10%\n",
    "\n",
    "print(\"Sample:\", next(iter(train_dataset)))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1188",
   "metadata": {},
   "source": [
    "## Step 2: Data Collation\n",
    "\n",
    "The **DataCollatorWithFlattening** is a special data collator from TRL that:\n",
    "- Handles variable-length sequences efficiently\n",
    "- Flattens conversation data for training\n",
    "- Optimizes memory usage during batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd11004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithFlattening\n",
    "\n",
    "data_collator = DataCollatorWithFlattening()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadbb67c",
   "metadata": {},
   "source": [
    "## Step 3: Loading Configuration\n",
    "\n",
    "We use a YAML configuration file to manage training parameters. This approach:\n",
    "- Keeps settings organized and version-controlled\n",
    "- Makes it easy to experiment with different hyperparameters\n",
    "- Allows reproducible training runs\n",
    "\n",
    "### Key Parameters Explained:\n",
    "- **base_model_name**: The foundation model to fine-tune\n",
    "- **lora_rank**: Controls the size of LoRA adapters (higher = more parameters)\n",
    "- **lora_alpha**: Scaling factor for LoRA updates (affects learning strength)\n",
    "- **batch_size**: Number of samples processed together\n",
    "- **epochs_to_train**: Number of complete passes through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3429860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56d3d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "LoRA adapter directory will be saved to: lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "LoRA rank is 16 and LoRA alpha is 32\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "MODEL_NAME = config[\"base_model_name\"]\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "\n",
    "adapter_dir = join(config[\"adapter_dir_prefix\"], MODEL_NAME)\n",
    "print(f\"LoRA adapter directory will be saved to: {adapter_dir}\")\n",
    "\n",
    "lora_rank = config[\"lora_rank\"]\n",
    "lora_alpha = config[\"lora_alpha\"]\n",
    "print(f\"LoRA rank is {lora_rank} and LoRA alpha is {lora_alpha}\")\n",
    "\n",
    "batch_size = int(config[\"batch_size\"])\n",
    "epochs_to_train = int(config[\"epochs_to_train\"])\n",
    "max_output_length = int(config[\"max_output_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520b0a1",
   "metadata": {},
   "source": [
    "## Step 4: Setting Up Model Components\n",
    "\n",
    "Now we'll import the essential libraries:\n",
    "\n",
    "- **PyTorch**: The underlying tensor computation framework\n",
    "- **PEFT (LoraConfig)**: Handles parameter-efficient fine-tuning\n",
    "- **Transformers**: Provides the model and tokenizer classes\n",
    "- **BitsAndBytesConfig**: Enables memory-efficient quantization\n",
    "- **TRL**: Provides the specialized trainer for supervised fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0998604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64344f8",
   "metadata": {},
   "source": [
    "## Step 5: Quantization Configuration\n",
    "\n",
    "**Quantization** reduces memory usage by representing model weights with fewer bits:\n",
    "\n",
    "- **load_in_4bit**: Use 4-bit precision instead of 16/32-bit (4x memory reduction!)\n",
    "- **bnb_4bit_compute_dtype**: Use bfloat16 for computations (stable training)\n",
    "- **bnb_4bit_use_double_quant**: Apply quantization twice for even more savings  \n",
    "- **bnb_4bit_quant_type**: \"nf4\" is an optimized 4-bit format\n",
    "\n",
    "This allows us to train larger models on consumer GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a4b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Changed from float16 to bfloat16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69632a48",
   "metadata": {},
   "source": [
    "## Step 6: Loading Model and Tokenizer\n",
    "\n",
    "Here we load the base model with several optimization flags:\n",
    "\n",
    "- **quantization_config**: Apply the 4-bit quantization we configured\n",
    "- **device_map=\"auto\"**: Automatically distribute model across available GPUs\n",
    "- **attn_implementation=\"flash_attention_2\"**: Use optimized attention for speed\n",
    "- **local_files_only**: Use cached models when available\n",
    "\n",
    "The tokenizer converts text to tokens the model can understand. We set `pad_token = eos_token` because some models don't have a dedicated padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee208692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,  # float16 to bfloat16\n",
    "    use_cache=True,  # Whether to cache attention outputs to speed up inference\n",
    "    quantization_config=bnb_config,\n",
    "    local_files_only=True,  # Use cache first\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    local_files_only=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c0e85",
   "metadata": {},
   "source": [
    "## Step 7: LoRA Configuration\n",
    "\n",
    "Now we configure the **LoRA adapter** - this is the heart of parameter-efficient fine-tuning!\n",
    "\n",
    "### LoRA Parameters Explained:\n",
    "- **r (rank)**: Size of the low-rank matrices\n",
    "- **lora_alpha**: Scaling factor for LoRA updates (higher = stronger adaptation)\n",
    "- **lora_dropout**: Prevents overfitting in the LoRA layers\n",
    "- **target_modules**: Which parts of the model to adapt (attention layers are most effective)\n",
    "\n",
    "### Why These Modules?\n",
    "We target the attention projection layers because they:\n",
    "- Control how the model focuses on different parts of the input\n",
    "- Are most impactful for learning new reasoning patterns\n",
    "- Provide good performance-to-parameter ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70becb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"self_attn.q_proj\", \"self_attn.v_proj\", \"self_attn.k_proj\", \"self_attn.o_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434daf1",
   "metadata": {},
   "source": [
    "## Step 8: Training Configuration\n",
    "\n",
    "The **SFTConfig** defines how we want to train our model:\n",
    "\n",
    "### Memory & Performance:\n",
    "- **gradient_accumulation_steps=4**: Process 4 batches before updating (saves memory)\n",
    "- **bf16=True**: Use bfloat16 precision (faster, stable training)\n",
    "- **gradient_checkpointing=True**: Trade compute for memory (essential for large models)\n",
    "\n",
    "### Training Strategy:\n",
    "- **completion_only_loss=True**: Only train on assistant responses, not user questions\n",
    "- **loss_type=\"dft\"**: Dynamic fine-tuning loss (TRL's improved loss function)\n",
    "\n",
    "### Monitoring:\n",
    "- **eval_strategy/save_strategy**: Save and evaluate every 100 steps\n",
    "- **logging_steps=50**: Log training metrics frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a6295e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SFT training parameters\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=config[\"training_results_dir\"],\n",
    "    num_train_epochs=epochs_to_train,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    max_length=max_output_length,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    loss_type=\"dft\",  # Dynamic fine tuning\n",
    "    completion_only_loss=True,  # Train only on assistant responses\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=False,  # Keep all columns for manual evaluation after training\n",
    "    report_to=\"mlflow\",  # Use MLflow to track training experiments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f379e23",
   "metadata": {},
   "source": [
    "## Step 9: Creating the SFT Trainer\n",
    "\n",
    "The **SFTTrainer** (Supervised Fine-Tuning Trainer) is TRL's specialized trainer that:\n",
    "- Handles conversation formatting automatically\n",
    "- Integrates with PEFT for LoRA training  \n",
    "- Provides optimized training loops for language model fine-tuning\n",
    "- Supports advanced features like completion-only training\n",
    "\n",
    "This single trainer handles all the complexity of modern LLM fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b668b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19e9d8",
   "metadata": {},
   "source": [
    "## Step 10: Memory Usage Check\n",
    "\n",
    "Before training, let's check our GPU memory usage to ensure we have enough resources:\n",
    "\n",
    "- **Memory allocated**: Currently used GPU memory\n",
    "- **Memory reserved**: Memory reserved by PyTorch for operations\n",
    "- **Memory available**: Total GPU memory capacity\n",
    "\n",
    "This helps us verify that our quantization and optimization settings are working correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c750bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory allocated: 0.11 GB\n",
      "GPU Memory reserved: 0.18 GB\n",
      "GPU Memory available: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory usage before training\n",
    "GB = 2**30\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / GB:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9734e",
   "metadata": {},
   "source": [
    "## Step 11: Start Training! ðŸš€\n",
    "\n",
    "This is where the magic happens! The trainer will:\n",
    "\n",
    "1. **Forward pass**: Run examples through the model\n",
    "2. **Compute loss**: Measure prediction accuracy  \n",
    "3. **Backward pass**: Calculate gradients\n",
    "4. **Update LoRA weights**: Apply parameter updates (only ~0.1% of total parameters!)\n",
    "5. **Evaluate**: Test on validation data periodically\n",
    "6. **Save checkpoints**: Store progress for recovery\n",
    "\n",
    "**Note**: Training progress will show loss decreasing and evaluation metrics improving. The beauty of LoRA is that we're only updating a tiny fraction of the model's parameters while achieving full fine-tuning performance!\n",
    "\n",
    "### Track experiment using MLFlow\n",
    "\n",
    "On the commandline run `pixi run -e cuda mlflow ui` to start the MLFlow tracking server. This will allow you to monitor the training in realtime at [http://localhost:5000](http://localhost:5000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19865571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/14 15:12:05 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/14 15:12:05 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "2026/01/14 15:12:05 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
      "2026/01/14 15:12:06 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
      "2026/01/14 15:12:07 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
      "2026/01/14 15:12:07 INFO alembic.runtime.migration: Running upgrade bf29a5ff90ea -> 1bd49d398cd23, add secrets tables\n",
      "2026/01/14 15:12:07 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/14 15:12:07 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4930' max='4930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4930/4930 13:11:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.150754</td>\n",
       "      <td>2.178163</td>\n",
       "      <td>1075029.000000</td>\n",
       "      <td>0.463330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.128782</td>\n",
       "      <td>1.628901</td>\n",
       "      <td>2151487.000000</td>\n",
       "      <td>0.478161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.103640</td>\n",
       "      <td>1.167220</td>\n",
       "      <td>3236559.000000</td>\n",
       "      <td>0.487163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.091400</td>\n",
       "      <td>0.088766</td>\n",
       "      <td>0.929870</td>\n",
       "      <td>4324382.000000</td>\n",
       "      <td>0.491260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.083448</td>\n",
       "      <td>0.852477</td>\n",
       "      <td>5401452.000000</td>\n",
       "      <td>0.494551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.080352</td>\n",
       "      <td>0.811891</td>\n",
       "      <td>6480808.000000</td>\n",
       "      <td>0.495975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.078513</td>\n",
       "      <td>0.787090</td>\n",
       "      <td>7567375.000000</td>\n",
       "      <td>0.497168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.077400</td>\n",
       "      <td>0.077125</td>\n",
       "      <td>0.769704</td>\n",
       "      <td>8649155.000000</td>\n",
       "      <td>0.498001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>0.759563</td>\n",
       "      <td>9737813.000000</td>\n",
       "      <td>0.498581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.075932</td>\n",
       "      <td>0.754449</td>\n",
       "      <td>10818350.000000</td>\n",
       "      <td>0.499037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.075768</td>\n",
       "      <td>0.753684</td>\n",
       "      <td>11898096.000000</td>\n",
       "      <td>0.499436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.075711</td>\n",
       "      <td>0.751322</td>\n",
       "      <td>12988354.000000</td>\n",
       "      <td>0.499740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.075575</td>\n",
       "      <td>0.749065</td>\n",
       "      <td>14070903.000000</td>\n",
       "      <td>0.499914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.075390</td>\n",
       "      <td>0.745810</td>\n",
       "      <td>15163034.000000</td>\n",
       "      <td>0.500108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.075292</td>\n",
       "      <td>0.744142</td>\n",
       "      <td>16233758.000000</td>\n",
       "      <td>0.500777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.075153</td>\n",
       "      <td>0.743620</td>\n",
       "      <td>17320319.000000</td>\n",
       "      <td>0.500887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.075119</td>\n",
       "      <td>0.742699</td>\n",
       "      <td>18408385.000000</td>\n",
       "      <td>0.500947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075119</td>\n",
       "      <td>0.743357</td>\n",
       "      <td>19485431.000000</td>\n",
       "      <td>0.501054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075061</td>\n",
       "      <td>0.743495</td>\n",
       "      <td>20562405.000000</td>\n",
       "      <td>0.500951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075046</td>\n",
       "      <td>0.742265</td>\n",
       "      <td>21639800.000000</td>\n",
       "      <td>0.500941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.075078</td>\n",
       "      <td>0.743370</td>\n",
       "      <td>22725971.000000</td>\n",
       "      <td>0.501101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.075062</td>\n",
       "      <td>0.743525</td>\n",
       "      <td>23813431.000000</td>\n",
       "      <td>0.501081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.075072</td>\n",
       "      <td>0.743128</td>\n",
       "      <td>24893081.000000</td>\n",
       "      <td>0.501066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075110</td>\n",
       "      <td>0.743425</td>\n",
       "      <td>25971629.000000</td>\n",
       "      <td>0.501136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.075065</td>\n",
       "      <td>0.742990</td>\n",
       "      <td>27054220.000000</td>\n",
       "      <td>0.501083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.075088</td>\n",
       "      <td>0.744037</td>\n",
       "      <td>28138341.000000</td>\n",
       "      <td>0.501163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075142</td>\n",
       "      <td>0.743782</td>\n",
       "      <td>29210164.000000</td>\n",
       "      <td>0.501187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.075136</td>\n",
       "      <td>0.744253</td>\n",
       "      <td>30299262.000000</td>\n",
       "      <td>0.501124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075059</td>\n",
       "      <td>0.743695</td>\n",
       "      <td>31385209.000000</td>\n",
       "      <td>0.501080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.075038</td>\n",
       "      <td>0.743422</td>\n",
       "      <td>32458674.000000</td>\n",
       "      <td>0.501172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.075088</td>\n",
       "      <td>0.743076</td>\n",
       "      <td>33538066.000000</td>\n",
       "      <td>0.501124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.075072</td>\n",
       "      <td>0.743009</td>\n",
       "      <td>34624304.000000</td>\n",
       "      <td>0.501263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.075095</td>\n",
       "      <td>0.743509</td>\n",
       "      <td>35706774.000000</td>\n",
       "      <td>0.501157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.075120</td>\n",
       "      <td>0.743659</td>\n",
       "      <td>36791218.000000</td>\n",
       "      <td>0.501027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075041</td>\n",
       "      <td>0.742538</td>\n",
       "      <td>37872503.000000</td>\n",
       "      <td>0.501148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.075107</td>\n",
       "      <td>0.743484</td>\n",
       "      <td>38955298.000000</td>\n",
       "      <td>0.501270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.075075</td>\n",
       "      <td>0.742220</td>\n",
       "      <td>40037189.000000</td>\n",
       "      <td>0.501124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.075089</td>\n",
       "      <td>0.742673</td>\n",
       "      <td>41117420.000000</td>\n",
       "      <td>0.501181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.075104</td>\n",
       "      <td>0.744116</td>\n",
       "      <td>42197036.000000</td>\n",
       "      <td>0.501118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075095</td>\n",
       "      <td>0.743008</td>\n",
       "      <td>43275025.000000</td>\n",
       "      <td>0.501094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.075103</td>\n",
       "      <td>0.743452</td>\n",
       "      <td>44355935.000000</td>\n",
       "      <td>0.501251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.075104</td>\n",
       "      <td>0.743542</td>\n",
       "      <td>45438654.000000</td>\n",
       "      <td>0.501212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.075145</td>\n",
       "      <td>0.743349</td>\n",
       "      <td>46520612.000000</td>\n",
       "      <td>0.501221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.075115</td>\n",
       "      <td>0.743257</td>\n",
       "      <td>47613714.000000</td>\n",
       "      <td>0.501198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075078</td>\n",
       "      <td>0.743006</td>\n",
       "      <td>48694059.000000</td>\n",
       "      <td>0.501143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.075106</td>\n",
       "      <td>0.742527</td>\n",
       "      <td>49778683.000000</td>\n",
       "      <td>0.501237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075115</td>\n",
       "      <td>0.743145</td>\n",
       "      <td>50860726.000000</td>\n",
       "      <td>0.501141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.075109</td>\n",
       "      <td>0.743107</td>\n",
       "      <td>51936398.000000</td>\n",
       "      <td>0.501122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.075112</td>\n",
       "      <td>0.743109</td>\n",
       "      <td>53024383.000000</td>\n",
       "      <td>0.501120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4930, training_loss=0.07982312873701053, metrics={'train_runtime': 47497.2835, 'train_samples_per_second': 1.659, 'train_steps_per_second': 0.104, 'total_flos': 3.457809136515456e+16, 'train_loss': 0.07982312873701053, 'entropy': 0.7427220115294824, 'num_tokens': 53338210.0, 'mean_token_accuracy': 0.5029056933191087, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb283ef7",
   "metadata": {},
   "source": [
    "## Step 12: Save the LoRA Adapter\n",
    "\n",
    "After training completes, we save our LoRA adapter:\n",
    "\n",
    "### What Gets Saved:\n",
    "- **LoRA weight matrices**: The small adapters we trained (~few MB)\n",
    "- **Adapter configuration**: Settings like rank, alpha, target modules\n",
    "- **Tokenizer**: Ensures consistency during inference\n",
    "\n",
    "### Why This is Amazing:\n",
    "- The base model stays unchanged (no need to duplicate GBs of weights)\n",
    "- Multiple LoRA adapters can be created for different tasks\n",
    "- Adapters can be easily shared, version-controlled, and swapped\n",
    "- You can even combine multiple LoRA adapters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ac35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA adapter to lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "LoRA adapter saved successfully to lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct/tokenizer_config.json',\n",
       " 'lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct/special_tokens_map.json',\n",
       " 'lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct/chat_template.jinja',\n",
       " 'lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct/vocab.json',\n",
       " 'lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct/merges.txt',\n",
       " 'lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct/added_tokens.json',\n",
       " 'lora_adapter/HuggingFaceTB/SmolLM2-135M-Instruct/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the LoRA adapter\n",
    "print(f\"Saving LoRA adapter to {adapter_dir}\")\n",
    "\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "print(f\"LoRA adapter saved successfully to {adapter_dir}!\")\n",
    "tokenizer.save_pretrained(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edcce60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_dataset_for_evaluation.<locals>.tokenize_function at 0x7afac1bf6840> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1ef05212c24f2cb24eb05049df63f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset for evaluation:   0%|          | 0/1971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='247' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [247/247 03:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Test Results:\n",
      "   eval_loss: 0.0750\n",
      "   eval_runtime: 215.6125\n",
      "   eval_samples_per_second: 9.1410\n",
      "   eval_steps_per_second: 1.1460\n",
      "   eval_entropy: 0.7423\n",
      "   eval_num_tokens: 53338210.0000\n",
      "   eval_mean_token_accuracy: 0.5010\n",
      "   epoch: 5.0000\n"
     ]
    }
   ],
   "source": [
    "# Process test dataset to match the expected format for evaluation\n",
    "# Apply the same chat template processing that SFTTrainer uses\n",
    "def process_dataset_for_evaluation(dataset, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        # Apply chat template to format messages\n",
    "        formatted_texts = []\n",
    "        for messages in examples[\"messages\"]:\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            formatted_texts.append(formatted_text)\n",
    "\n",
    "        # Tokenize the formatted text\n",
    "        tokenized = tokenizer(\n",
    "            formatted_texts,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=trainer.args.max_length,\n",
    "            return_overflowing_tokens=False,\n",
    "        )\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    # Process the dataset\n",
    "    processed_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing dataset for evaluation\",\n",
    "    )\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "processed_test_dataset = process_dataset_for_evaluation(test_dataset, tokenizer)\n",
    "\n",
    "test_results = trainer.evaluate(processed_test_dataset)\n",
    "print(\"ðŸŽ¯ Test Results:\")\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2390d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Now run the notebook `trl_medical_reasoning_inference.ipynb` to use the LoRA fine-tuned model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cf394",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. âœ… Learned about LoRA and TRL\n",
    "2. âœ… Loaded and preprocessed medical reasoning data  \n",
    "3. âœ… Set up memory-efficient quantization\n",
    "4. âœ… Configured LoRA adapters for parameter-efficient training\n",
    "5. âœ… Fine-tuned a language model using TRL's SFTTrainer\n",
    "6. âœ… Saved your trained LoRA adapter\n",
    "\n",
    "### Next Steps:\n",
    "- Use the inference notebook to test your fine-tuned model\n",
    "- Experiment with different LoRA ranks and alphas\n",
    "- Try training on different datasets\n",
    "- Combine multiple LoRA adapters for multi-task models\n",
    "\n",
    "### Key Takeaways:\n",
    "- **LoRA** enables efficient fine-tuning with minimal memory\n",
    "- **TRL** provides state-of-the-art training techniques\n",
    "- **Quantization** makes large models accessible on consumer hardware\n",
    "- **Parameter-efficient fine-tuning** is the future of model customization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
