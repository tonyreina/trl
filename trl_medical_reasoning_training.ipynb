{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: {'messages': [{'content': 'A patient presents with microcytic hypochromic anemia, hemoglobin level of 9%, serum iron of 20 µg/dL, ferritin level of 800 ng/mL, and transferrin percentage saturation of 64%. Based on these laboratory findings, what is the possible diagnosis?', 'role': 'user'}, {'content': \"<think>Okay, so we have a case of microcytic hypochromic anemia. That generally means the red blood cells are small and pale, which can occur in a few different conditions.\\n\\nLet's start by looking at the serum iron level. It’s reported at 20 µg/dL, which is definitely on the low side. Low serum iron is commonly seen in iron deficiency anemia, but it can also happen due to chronic diseases or other less common conditions.\\n\\nNext, there’s the ferritin level to consider. It’s really high at 800 ng/mL. Ferritin being high makes me think more about conditions like an inflammatory state or chronic disease, rather than iron deficiency, where ferritin would typically be low.\\n\\nNow, looking at the transferrin saturation, it’s sitting at 64%, which is quite high. That's a bit puzzling because in iron deficiency anemia, I would expect this to be low. This high saturation suggests there's more than enough iron present, which doesn’t match up with a straightforward iron deficiency.\\n\\nWhen I put this all together, low serum iron, high ferritin, and high transferrin saturation do not classically fit with what's expected for the typical causes of microcytic hypochromic anemia, like iron deficiency anemia or anemia of chronic disease.\\n\\nI wonder if there’s more going on here. Something like anemia of chronic disease crossed my mind at first, but with this high transferrin saturation, it doesn't really fit.\\n\\nHmm, could it be a case of sideroblastic anemia? But the high transferrin saturation seems off as sideroblastic anemia can show variable saturation, often leaning lower.\\n\\nLooking at the big picture again, excess iron is suggested by the high ferritin, so maybe something related to iron overload, like hemochromatosis. But then, the low serum iron perplexes me when it comes to hemochromatosis.\\n\\nCould this be an unusual transporter issue? Like atransferrinemia? That condition could lead to poor iron usage even though there's plenty of iron available, explaining the high saturation and ferritin.\\n\\nThat idea does make a lot of sense given what I'm seeing. Everything points to a problem with how iron is being managed in the body, rather than a classic case of deficiency or overload.\\n\\nSo, now, putting it all together, I think atransferrinemia, a rare disorder where the body doesn't handle iron transport well, fits the data. Something pretty rare, but it explains these unusual lab results well.</think>Based on the laboratory findings and the analysis of the situation, the possible diagnosis for this patient is atransferrinemia. Atransferrinemia is a rare disorder characterized by an abnormality in iron transport, leading to inefficient iron distribution and utilization in the body. This diagnosis aligns with the lab results showing low serum iron, high ferritin levels, and high transferrin saturation. These features suggest an issue with iron transport rather than a deficiency or overload, which is consistent with atransferrinemia.\", 'role': 'assistant'}]}\n",
      "Training samples: 15763\n",
      "Test samples: 1971\n",
      "Validation samples: 1970\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "\n",
    "\n",
    "def preprocess_function(example):\n",
    "    # Format as a conversation for SFTTrainer\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"Question\"]},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"<think>{example['Complex_CoT']}</think>{example['Response']}\",\n",
    "        },\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, remove_columns=[\"Question\", \"Response\", \"Complex_CoT\"])\n",
    "\n",
    "# Split the training dataset to create train/validation/test sets\n",
    "# (80% train, 10% validation, 10% test)\n",
    "first_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=816)  # 80% train, 20% temp\n",
    "temp_dataset = first_split[\"test\"]\n",
    "second_split = temp_dataset.train_test_split(test_size=0.5, seed=816)  # Split the 20% into 10% each\n",
    "\n",
    "train_dataset = first_split[\"train\"]  # 80%\n",
    "eval_dataset = second_split[\"train\"]  # 10%\n",
    "test_dataset = second_split[\"test\"]  # 10%\n",
    "\n",
    "print(\"Sample:\", next(iter(train_dataset)))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3429860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d3d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: HuggingFaceTB/SmolLM-135M-Instruct\n",
      "LoRA adapter directory will be saved to: lora_adapter/HuggingFaceTB/SmolLM-135M-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "MODEL_NAME = config[\"base_model_name\"]\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "\n",
    "adapter_dir = join(config[\"adapter_dir_prefix\"], MODEL_NAME)\n",
    "print(f\"LoRA adapter directory will be saved to: {adapter_dir}\")\n",
    "\n",
    "lora_rank = config[\"lora_rank\"]\n",
    "lora_alpha = config[\"lora_alpha\"]\n",
    "print(f\"LoRA rank is {lora_rank} and LoRA alpha is {lora_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0998604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a4b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Changed from float16 to bfloat16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee208692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,  # Changed from float16 to bfloat16 and dtype to torch_dtype\n",
    "    use_cache=True,  # Whether to cache attention outputs to speed up inference\n",
    "    quantization_config=bnb_config,\n",
    "    local_files_only=True,  # Use cache first\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    local_files_only=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70becb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"self_attn.q_proj\", \"self_attn.v_proj\", \"self_attn.k_proj\", \"self_attn.o_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6295e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SFT training parameters\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    max_length=512,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_steps=20,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    loss_type=\"dft\",  # Dynamic fine tuning\n",
    "    completion_only_loss=True,  # Train only on assistant responses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd11004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithFlattening\n",
    "\n",
    "data_collator = DataCollatorWithFlattening()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b668b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c750bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory allocated: 0.11 GB\n",
      "GPU Memory reserved: 0.18 GB\n",
      "GPU Memory available: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory usage before training\n",
    "GB = 2**30\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / GB:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19865571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='493' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101/493 07:04 < 28:01, 0.23 it/s, Epoch 0.20/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.144826</td>\n",
       "      <td>2.234576</td>\n",
       "      <td>320470.000000</td>\n",
       "      <td>0.449162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.143242</td>\n",
       "      <td>2.178591</td>\n",
       "      <td>640524.000000</td>\n",
       "      <td>0.450605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>0.141499</td>\n",
       "      <td>2.120491</td>\n",
       "      <td>962519.000000</td>\n",
       "      <td>0.452169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.139755</td>\n",
       "      <td>2.066365</td>\n",
       "      <td>1284493.000000</td>\n",
       "      <td>0.453770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/247 00:01 < 00:51, 4.58 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trl/.pixi/envs/cuda/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trl/.pixi/envs/cuda/lib/python3.13/site-packages/transformers/trainer.py:2756\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2754\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m) / steps_in_epoch\n\u001b[32m   2755\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2756\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2767\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trl/.pixi/envs/cuda/lib/python3.13/site-packages/transformers/trainer.py:3221\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3219\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3221\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3222\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trl/.pixi/envs/cuda/lib/python3.13/site-packages/transformers/trainer.py:3170\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3170\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3171\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3173\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trl/.pixi/envs/cuda/lib/python3.13/site-packages/transformers/trainer.py:4489\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4486\u001b[39m start_time = time.time()\n\u001b[32m   4488\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4489\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4490\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4492\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4499\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trl/.pixi/envs/cuda/lib/python3.13/site-packages/transformers/trainer.py:4685\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4682\u001b[39m         batch_size = observed_batch_size\n\u001b[32m   4684\u001b[39m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4685\u001b[39m losses, logits, labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4686\u001b[39m main_input_name = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmain_input_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4687\u001b[39m inputs_decode = (\n\u001b[32m   4688\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4689\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trl/.pixi/envs/cuda/lib/python3.13/site-packages/transformers/trainer.py:4902\u001b[39m, in \u001b[36mTrainer.prediction_step\u001b[39m\u001b[34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[39m\n\u001b[32m   4900\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m   4901\u001b[39m     num_items_in_batch = \u001b[38;5;28mself\u001b[39m._get_num_items_in_batch([inputs], \u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m-> \u001b[39m\u001b[32m4902\u001b[39m     loss, outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4903\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[32m   4904\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4905\u001b[39m loss = loss.detach().mean()\n\u001b[32m   4907\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/trl/.pixi/envs/cuda/lib/python3.13/site-packages/trl/trainer/sft_trainer.py:1175\u001b[39m, in \u001b[36mSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   1173\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1174\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mExpected \u001b[39m\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mposition_ids\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in inputs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m         entropy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather_for_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentropy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m     \u001b[38;5;28mself\u001b[39m._metrics[mode][\u001b[33m\"\u001b[39m\u001b[33mentropy\u001b[39m\u001b[33m\"\u001b[39m].append(entropy)\n\u001b[32m   1178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1179\u001b[39m     \u001b[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[32m   1180\u001b[39m     \u001b[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "print(f\"Saving LoRA adapter to {adapter_dir}\")\n",
    "\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "print(f\"LoRA adapter saved successfully to {adapter_dir}!\")\n",
    "tokenizer.save_pretrained(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2390d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now run the notebook `trl_medical_reasoning_inference.ipynb` to use the LoRA fine-tuned model.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Now run the notebook `trl_medical_reasoning_inference.ipynb` to use the LoRA fine-tuned model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a933ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
