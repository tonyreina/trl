base_model_name: HuggingFaceTB/SmolLM2-135M-Instruct
adapter_dir_prefix: lora_adapter
training_results_dir: results
lora_rank: 16
lora_alpha: 32  # Usually double the rank
batch_size: 4
epochs_to_train: 1
max_output_length: 1024
