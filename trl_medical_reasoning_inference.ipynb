{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce60692",
   "metadata": {},
   "source": [
    "# Inference with TRL LoRA adapter model\n",
    "\n",
    "> NOTE\n",
    "> ----\n",
    "> Run the `trl_medical_reasoning_training.ipynb` first to fine-tune the model.\n",
    "\n",
    "> Important\n",
    "> ---------\n",
    "> This notebook is for educational purposes only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117898e2",
   "metadata": {},
   "source": [
    "## Model Loading and Setup\n",
    "\n",
    "This section loads both the base model and the LoRA-adapted model for comparison:\n",
    "\n",
    "1. **Configuration Loading**: Reads model settings from `config.yaml`\n",
    "2. **Base Model Loading**: Loads the original pre-trained model without any fine-tuning\n",
    "3. **LoRA Model Loading**: Loads the same base model and applies the LoRA adapter weights\n",
    "4. **Tokenizer Setup**: Configures the tokenizer for text processing\n",
    "\n",
    "The key difference is that we load the base model twice:\n",
    "- `base_model`: The original model for baseline comparisons\n",
    "- `lora_model`: The same model enhanced with LoRA adapter for medical reasoning\n",
    "\n",
    "Both models use the same tokenizer and are loaded with `bfloat16` precision for efficient GPU memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both base model and LoRA adapter for comparison\n",
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load configuration from config.yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "MODEL_NAME = config[\"base_model_name\"]\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "\n",
    "adapter_dir = join(config[\"adapter_dir_prefix\"], MODEL_NAME)\n",
    "print(f\"LoRA adapter directory: {adapter_dir}\")\n",
    "\n",
    "max_output_length = int(config[\"max_output_length\"])\n",
    "\n",
    "# Load the base model for inference (without LoRA)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "# Load the base model again and add LoRA adapter\n",
    "lora_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    lora_model,\n",
    "    adapter_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer for inference\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    adapter_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "if inference_tokenizer.pad_token is None:\n",
    "    inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "\n",
    "print(\"Both base model and LoRA adapter loaded successfully for comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a63a8",
   "metadata": {},
   "source": [
    "## Response Generation Functions\n",
    "\n",
    "This section defines the core inference functions for medical reasoning:\n",
    "\n",
    "### Key Functions:\n",
    "\n",
    "1. **`generate_medical_response_single()`**: \n",
    "   - Generates responses from a single model (base or LoRA)\n",
    "   - Handles chat templating, tokenization, and response generation\n",
    "   - Uses configurable parameters for temperature and sampling\n",
    "\n",
    "2. **`generate_medical_response_comparison()`**: \n",
    "   - Generates responses from both base and LoRA models\n",
    "   - Returns both responses for side-by-side comparison\n",
    "   - Shows the difference in medical reasoning capabilities\n",
    "\n",
    "3. **`generate_medical_response()`**: \n",
    "   - Convenience function that uses only the LoRA model\n",
    "   - Maintains backward compatibility with existing code\n",
    "\n",
    "The comparison approach allows you to see how the LoRA fine-tuning improves medical reasoning compared to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde84b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medical_response_single(\n",
    "    model, question, max_new_tokens=1024, temperature=0.7, do_sample=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a medical reasoning response for a given question using a specific model.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use for generation\n",
    "        question (str): The medical question to answer\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate\n",
    "        temperature (float): Temperature for sampling (higher = more creative)\n",
    "        do_sample (bool): Whether to use sampling or greedy decoding\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response with reasoning\n",
    "    \"\"\"\n",
    "    # Format the input as a conversation with system prompt for CoT\n",
    "    # Note: SmolLM3-3B requires \"/think\" at the end of system prompt to enable extended thinking\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a medical AI assistant. \"\n",
    "            \"When answering medical questions, use /think \"\n",
    "            \"to show your reasoning process before providing \"\n",
    "            \" your final answer. Structure your response as: \"\n",
    "            \"/think [your detailed reasoning] [final answer]./think\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    # Apply chat template (following official SmolLM3-3B example)\n",
    "    input_text = inference_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the input (following official example format)\n",
    "    model_inputs = inference_tokenizer([input_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=inference_tokenizer.pad_token_id,\n",
    "            eos_token_id=inference_tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    # Extract only the generated part (following official example)\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\n",
    "    response = inference_tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def generate_medical_response_comparison(\n",
    "    question, max_new_tokens=1024, temperature=0.7, do_sample=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate medical reasoning responses from both base model and LoRA adapter for comparison.\n",
    "\n",
    "    Args:\n",
    "        question (str): The medical question to answer\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate\n",
    "        temperature (float): Temperature for sampling (higher = more creative)\n",
    "        do_sample (bool): Whether to use sampling or greedy decoding\n",
    "\n",
    "    Returns:\n",
    "        tuple: (base_response, lora_response) - responses from base model and LoRA adapter\n",
    "    \"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"Generating response from base model...\")\n",
    "    base_response = generate_medical_response_single(\n",
    "        base_model, question, max_new_tokens, temperature, do_sample\n",
    "    )\n",
    "\n",
    "    print(\"Generating response from LoRA adapter...\")\n",
    "    lora_response = generate_medical_response_single(\n",
    "        lora_model, question, max_new_tokens, temperature, do_sample\n",
    "    )\n",
    "\n",
    "    return base_response, lora_response\n",
    "\n",
    "\n",
    "def generate_medical_response(question, max_new_tokens=1024, temperature=0.7, do_sample=True):\n",
    "    \"\"\"\n",
    "    Generate a medical reasoning response using the LoRA adapter (backward compatibility).\n",
    "    \"\"\"\n",
    "    return generate_medical_response_single(\n",
    "        lora_model, question, max_new_tokens, temperature, do_sample\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Medical response generation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296804a",
   "metadata": {},
   "source": [
    "## Interactive Medical Chat Interface\n",
    "\n",
    "This section provides an interactive chat interface for medical questions:\n",
    "\n",
    "### Features:\n",
    "- **Real-time Comparison**: Shows responses from both base and LoRA models simultaneously\n",
    "- **Interactive Input**: Type medical questions and get immediate responses\n",
    "- **Visual Formatting**: Clearly distinguishes between base model (ðŸ”µ) and LoRA adapter (ðŸŸ ) responses\n",
    "- **Error Handling**: Gracefully handles errors and allows you to continue chatting\n",
    "- **Easy Exit**: Type 'quit', 'exit', or 'q' to stop the chat session\n",
    "\n",
    "This is perfect for exploring how the LoRA fine-tuning affects medical reasoning quality in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24caf8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive inference function with comparison\n",
    "def interactive_medical_chat():\n",
    "    \"\"\"\n",
    "    Interactive chat function for medical questions showing both base model and LoRA responses.\n",
    "    Type 'quit' or 'exit' to stop.\n",
    "    \"\"\"\n",
    "    print(\"=== Medical Reasoning Assistant - Model Comparison ===\")\n",
    "    print(\n",
    "        \"Ask me medical questions! I'll show responses from both the base model and LoRA adapter.\"\n",
    "    )\n",
    "    print(\"Type 'quit' or 'exit' to stop.\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            question = input(\"You: \").strip()\n",
    "\n",
    "            # Check for exit commands\n",
    "            if question.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "\n",
    "            if not question:\n",
    "                print(\"Please enter a question.\")\n",
    "                continue\n",
    "\n",
    "            print(\"\\nProcessing...\")\n",
    "\n",
    "            # Generate responses from both models\n",
    "            base_response, lora_response = generate_medical_response_comparison(\n",
    "                question, max_new_tokens=max_output_length\n",
    "            )\n",
    "\n",
    "            print(f\"Question: {question}\")\n",
    "\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"ðŸ”µ BASE MODEL RESPONSE:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"{base_response}\")\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"ðŸŸ  LORA ADAPTER RESPONSE:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"{lora_response}\")\n",
    "            print(f\"\\n{'='*60}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Please try again.\")\n",
    "\n",
    "\n",
    "print(\"Interactive medical chat function with comparison defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c57190",
   "metadata": {},
   "source": [
    "## Automated Testing with Sample Questions\n",
    "\n",
    "> NOTE\n",
    "> In the default notebook we only train for 1 epoch.\n",
    "> This won't be sufficient to produce a fine-tuned model\n",
    "> that is substantially better than the base.\n",
    "\n",
    "This section runs a comprehensive test suite to evaluate both models:\n",
    "\n",
    "### Test Questions Include:\n",
    "- **Symptom Recognition**: \"What are the common symptoms of diabetes?\"\n",
    "- **Treatment Planning**: \"How should I treat a patient with hypertension?\"\n",
    "- **Diagnostic Procedures**: \"What diagnostic tests should be ordered for chest pain?\"\n",
    "- **Pharmacology**: \"Explain the mechanism of action of ACE inhibitors.\"\n",
    "- **Clinical Guidelines**: \"What are the contraindications for aspirin therapy?\"\n",
    "\n",
    "### What to Look For:\n",
    "- **Depth of Reasoning**: How detailed and structured are the responses?\n",
    "- **Medical Accuracy**: Are the medical facts correct and up-to-date?\n",
    "- **Clinical Relevance**: Do the responses address practical clinical scenarios?\n",
    "- **Consistency**: Are similar questions answered with consistent quality?\n",
    "\n",
    "The side-by-side comparison helps you quantify the improvement from LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the inference with sample questions - showing both models\n",
    "test_questions = [\n",
    "    \"What are the common symptoms of diabetes?\",\n",
    "    \"How should I treat a patient with hypertension?\",\n",
    "    \"What diagnostic tests should be ordered for chest pain?\",\n",
    "    \"Explain the mechanism of action of ACE inhibitors.\",\n",
    "    \"What are the contraindications for aspirin therapy?\",\n",
    "]\n",
    "\n",
    "print(\"=== Testing Medical Reasoning Model - Base vs LoRA Comparison ===\\n\")\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TEST QUESTION {i}: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    try:\n",
    "        base_response, lora_response = generate_medical_response_comparison(\n",
    "            question, max_new_tokens=max_output_length, temperature=0.7\n",
    "        )\n",
    "\n",
    "        print(\"ðŸ”µ BASE MODEL RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{base_response}\")\n",
    "        print()\n",
    "\n",
    "        print(\"ðŸŸ  LORA ADAPTER RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{lora_response}\")\n",
    "        print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "\n",
    "    print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3dcce",
   "metadata": {},
   "source": [
    "## Usage Examples and API Guide\n",
    "\n",
    "This section demonstrates different ways to use the inference functions:\n",
    "\n",
    "### Available Methods:\n",
    "\n",
    "1. **Comparison Mode** (Recommended):\n",
    "   ```python\n",
    "   base_resp, lora_resp = generate_medical_response_comparison('Your question here')\n",
    "   ```\n",
    "\n",
    "2. **LoRA Only Mode**:\n",
    "   ```python\n",
    "   response = generate_medical_response('Your question here')\n",
    "   ```\n",
    "\n",
    "3. **Interactive Chat**:\n",
    "   ```python\n",
    "   interactive_medical_chat()\n",
    "   ```\n",
    "\n",
    "### Parameters:\n",
    "- `max_new_tokens`: Controls maximum new tokens to generate (default: from config)\n",
    "- `temperature`: Controls creativity (0.1 = conservative, 1.0 = creative)\n",
    "- `do_sample`: Use sampling vs. greedy decoding\n",
    "\n",
    "Choose the method that best fits your use case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03454b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start interactive chat (uncomment to use)\n",
    "# interactive_medical_chat()\n",
    "\n",
    "# Alternative: Direct question answering with comparison\n",
    "print(\"\\n=== Direct Question Answering with Comparison ===\")\n",
    "print(\"You can now compare responses from both models:\")\n",
    "print(\n",
    "    \"Example: \"\n",
    "    \"base_resp, lora_resp = \"\n",
    "    \"generate_medical_response_comparison('What causes high blood pressure?')\"\n",
    ")\n",
    "print(\"Then: print('Base:', base_resp)\")\n",
    "print(\"      print('LoRA:', lora_resp)\")\n",
    "print(\"\\nOr for single LoRA response (backward compatibility):\")\n",
    "print(\"response = generate_medical_response('What causes high blood pressure?')\")\n",
    "print(\"\\nRun interactive_medical_chat() for a continuous conversation with comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9becc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demo of base vs LoRA comparison\n",
    "demo_question = \"What are the main symptoms of Type 2 diabetes?\"\n",
    "\n",
    "print(\"=== DEMO: Base Model vs LoRA Adapter Comparison ===\")\n",
    "print(f\"Question: {demo_question}\\n\")\n",
    "\n",
    "base_response, lora_response = generate_medical_response_comparison(\n",
    "    demo_question, max_new_tokens=300, temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"ðŸ”µ BASE MODEL:\")\n",
    "print(\"-\" * 20)\n",
    "print(base_response)\n",
    "print()\n",
    "\n",
    "print(\"ðŸŸ  LORA ADAPTER:\")\n",
    "print(\"-\" * 20)\n",
    "print(lora_response)\n",
    "print(\"\\nNotice the differences in medical reasoning depth and structure!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8747de",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_medical_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
