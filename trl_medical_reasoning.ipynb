{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "\n",
    "\n",
    "def preprocess_function(example):\n",
    "    # Format as a conversation for SFTTrainer\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"Question\"]},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"<think>{example['Complex_CoT']}</think>{example['Response']}\",\n",
    "        },\n",
    "    ]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, remove_columns=[\"Question\", \"Response\", \"Complex_CoT\"])\n",
    "\n",
    "# Split the training dataset to create train/validation/test sets\n",
    "# (80% train, 10% validation, 10% test)\n",
    "first_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=816)  # 80% train, 20% temp\n",
    "temp_dataset = first_split[\"test\"]\n",
    "second_split = temp_dataset.train_test_split(test_size=0.5, seed=816)  # Split the 20% into 10% each\n",
    "\n",
    "train_dataset = first_split[\"train\"]  # 80%\n",
    "eval_dataset = second_split[\"train\"]  # 10%\n",
    "test_dataset = second_split[\"test\"]  # 10%\n",
    "\n",
    "print(\"Sample:\", next(iter(train_dataset)))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3429860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a4b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Changed from float16 to bfloat16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee208692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,  # Changed from float16 to bfloat16 and dtype to torch_dtype\n",
    "    use_cache=True,  # Whether to cache attention outputs to speed up inference\n",
    "    quantization_config=bnb_config,\n",
    "    local_files_only=True,  # Use cache first\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    local_files_only=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70becb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"self_attn.q_proj\", \"self_attn.v_proj\", \"self_attn.k_proj\", \"self_attn.o_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6295e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SFT training parameters\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    max_length=512,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_steps=20,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    loss_type=\"dft\",  # Dynamic fine tuning\n",
    "    completion_only_loss=True,  # Train only on assistant responses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd11004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithFlattening\n",
    "\n",
    "data_collator = DataCollatorWithFlattening()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b668b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c750bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage before training\n",
    "GB = 2**30\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved() / GB:.2f} GB\")\n",
    "    print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / GB:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19865571",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "adapter_dir = join(\"lora_adapter\", MODEL_NAME)\n",
    "print(f\"Saving LoRA adapter to {adapter_dir}\")\n",
    "\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "print(\"LoRA adapter saved successfully!\")\n",
    "tokenizer.save_pretrained(adapter_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
